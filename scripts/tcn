import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
import ml_helper
import torch
import os
from torch import Tensor
import numpy as np
import torch.nn as nn
import torch.optim as optim


# -- explains every step of the code --
#https://de.mathworks.com/help/deeplearning/ug/sequence-to-sequence-classification-using-1-d-convolutions.html

# TODO: check without padding so variable sequence length

# NOTE: check if needed
# Check if CUDA (GPU support) is available
# if torch.cuda.is_available():
#     # GPU is available
#     device = torch.device("cuda")
#     print("GPU is available. Using GPU for computations.")
# else:
#     # GPU is not available, fallback to CPU
#     device = torch.device("cpu")
#     print("GPU is not available. Using CPU for computations.")

# NOTE: start this script from the scripts directory
# check from which directory the script is run
print(os.getcwd())


# inherit from codon dataset
class CodonDataset_CNN(ml_helper.CodonDataset):
    def __init__(self, organism: str):
        super().__init__(organism)

    def __getitem__(self, idx):
        data = super().__getitem__(idx)
        return data
    
    def __len__(self):
        return super().__len__()

    def transform_input(self, look_back: int, look_forward: int) -> Tensor:
        min_len = 200
        max_len = 500

        x_data = []
        y_data = []
        # create a input window like a rowlling window
        for idx, row in self.df.iterrows():
            seq = row["translation"].seq
            #if seq is over 400 or under 300 skip the sequence
            if len(seq) >= max_len or len(seq) < min_len:
                continue
            aa_sequence = ml_helper.aa_to_onehot_tensor(seq)
            
            # calculate the missing padding at the end to get to max length
            forward_padding = max_len - len(aa_sequence) + look_forward
            # TODO: zeros or -1?
            padded_aa_sequence = np.pad(aa_sequence, ((look_back, forward_padding), (0, 0)), 'constant', constant_values=0)
            # split the padded aa sequence into a parts of rows with the length of look_back + 1 + look_forward
            aa_sequence = [padded_aa_sequence[i:i + look_back + 1 + look_forward] for i in range(len(padded_aa_sequence) - look_forward - look_back)]
            aa_sequence = torch.tensor(np.array(aa_sequence))
            x_data.append(aa_sequence)

            codon_sequence = row["sequence"]
            codon_sequence = ml_helper.codon_to_tensor(codon_sequence)
            # add the padded codon sequence to the y_data
            # TODO: padding categories are len(ml_helper.codons) or -1
            forward_padding= forward_padding - look_forward
            padded_codon_sequence = np.pad(codon_sequence, (0, forward_padding), 'constant', constant_values=-1)
            y_data.append(torch.tensor(padded_codon_sequence))

        x_data = torch.stack(x_data)
        y_data = torch.stack(y_data)
        return x_data, y_data


# Create the dataset for a specific organism
organism = "E.Coli"  # replace with the organism you want

dataset = CodonDataset_CNN(organism)


l_b = 0
l_f = 0
input_dataset = dataset.transform_input(look_back=l_b, look_forward=l_f)
sample_size = len(input_dataset[0])

# shuffle the dataset
shuffled_indices = torch.randperm(sample_size)
x_data = input_dataset[0][shuffled_indices]
y_data = input_dataset[1][shuffled_indices]

if l_b == 0 and l_f == 0:
    x_data = x_data.squeeze(dim=2)
    print(f" Flattend Shape of the dataset: {x_data.shape}")

# split the dataset into training and validation sets
train_size = int(0.8 * sample_size)
val_size = sample_size - train_size
x_train = x_data[:train_size]
y_train = y_data[:train_size]
x_val = x_data[train_size:]
y_val = y_data[train_size:]

# Define the TCN model
class TemporalConvNet(nn.Module):
    def __init__(self, num_features, num_classes, num_filters, filter_size, dropout_factor, num_blocks):
        super(TemporalConvNet, self).__init__()
        self.num_blocks = num_blocks
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.dropout_factor = dropout_factor
        
        self.layers = nn.ModuleList()
        
        # Input layer
        self.layers.append(nn.Conv1d(num_features, num_filters, kernel_size=1))
        
        # Dilated convolutional blocks
        for i in range(num_blocks):
            dilation = 2 ** i

            # this function defines the padding and therefore teh sequence length
            pad = (filter_size - 1) * dilation // 2 
            # TODO: fix for even filter size

            self.layers.append(nn.Conv1d(num_filters, num_filters, kernel_size=filter_size, dilation=dilation, padding=pad))
            self.layers.append(nn.BatchNorm1d(num_filters))
            self.layers.append(nn.ReLU())

            # manually added
            #self.layers.append(nn.Conv1d(num_filters, num_classes, kernel_size=1))
            self.layers.append(nn.Conv1d(num_filters, num_filters, kernel_size=filter_size, dilation=dilation, padding=pad))
            self.layers.append(nn.BatchNorm1d(num_filters))
            #self.layers.append(nn.ReLU())


            self.layers.append(nn.Dropout(dropout_factor))
            self.layers.append(nn.ReLU())
        
        # Output layer (not flatten and dense as this would aggregate the sequence information)
        self.layers.append(nn.Conv1d(num_filters, num_classes, kernel_size=1))
        self.layers.append(nn.Softmax(dim=1))

    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# Create DataLoader
train_dataset = TensorDataset(x_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Define model parameters
num_features = x_train.shape[2]
num_classes = 64  # number of codons (output classes)
num_filters = 64 #64
filter_size = 3 #=15  # NOTE: filter size must be unequal like: 3,6,9,...
dropout_factor = 0.005 #0.005
num_blocks = 2

#num_blocks , filter_size = accuracy
"""

3, 5  = 0.34

10, 3 = 0.207
5, 7 = 0.37
5, 3 = 0.415
5,1 = 0.4127
3, 3  = 0.34
2, 3 = 0.4255
1, 3 = 0.42
"""


print(f'Number of features: {num_features}, Number of classes: {num_classes}')

# Initialize and train the model
model = TemporalConvNet(num_features, num_classes, num_filters, filter_size, dropout_factor, num_blocks)
criterion = nn.CrossEntropyLoss(ignore_index=-1)
optimizer = optim.Adam(model.parameters(), lr=0.005) # lr=0.001

# Training loop
num_epochs = 20
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        """
         "NCHW":
            N represents the batch size.
            C represents the number of channels (features).
            H represents the height (or length in 1D convolution).
            W represents the width (for 2D convolution).
        """
        outputs = model(inputs.permute(0, 2, 1))  # PyTorch expects channel-first format .permute(0, 2, 1)

        loss = criterion(outputs.squeeze(), labels.squeeze().long())
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}')

XTest = x_val.clone().detach().float()
TTest = y_val.clone().detach().long()

with torch.no_grad():
    outputs = model(XTest.permute(0, 2, 1))  # PyTorch expects channel-first format
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == TTest.squeeze()).sum().item() / (TTest.size(0) * TTest.size(0))
    print(f'Test Accuracy: {accuracy}')


# TODO: configurate plots and confusion matrix
"""
# Plotting predictions
plt.figure()
plt.plot(predicted.numpy(), '.-', label='Predicted')
plt.plot(TTest.numpy(), label='Test Data')
plt.xlabel('Time Step')
plt.ylabel('Activity')
plt.legend()
plt.title('Test Sequence Predictions')
plt.show()

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(TTest.numpy(), predicted.numpy())
print('Confusion Matrix:')
print(cm)
"""