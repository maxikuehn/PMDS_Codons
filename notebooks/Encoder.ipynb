{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only Transformer Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "sys.path.append('../scripts')\n",
    "import ml_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test and Training Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', '*',\n",
    "               '_']\n",
    "\n",
    "aminoacids_to_integer = dict((a, i) for i, a in enumerate(amino_acids))\n",
    "integer_to_aminoacids = dict((i, a) for i, a in enumerate(amino_acids))\n",
    "\n",
    "codons = ['TTT', 'TTC', 'TTA', 'TTG', 'TCT', 'TCC', 'TCA', 'TCG', 'TAT', 'TAC', 'TAA', 'TAG', 'TGT', 'TGC', 'TGA',\n",
    "          'TGG', 'CTT', 'CTC', 'CTA', 'CTG', 'CCT', 'CCC', 'CCA', 'CCG', 'CAT', 'CAC', 'CAA', 'CAG', 'CGT', 'CGC',\n",
    "          'CGA', 'CGG', 'ATT', 'ATC', 'ATA', 'ATG', 'ACT', 'ACC', 'ACA', 'ACG', 'AAT', 'AAC', 'AAA', 'AAG', 'AGT',\n",
    "          'AGC', 'AGA', 'AGG', 'GTT', 'GTC', 'GTA', 'GTG', 'GCT', 'GCC', 'GCA', 'GCG', 'GAT', 'GAC', 'GAA', 'GAG',\n",
    "          'GGT', 'GGC', 'GGA', 'GGG', '___']\n",
    "\n",
    "codons_to_integer = dict((c, i) for i, c in enumerate(codons))\n",
    "integer_to_codons = dict((i, c) for i, c in enumerate(codons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\insab\\OneDrive\\_Dokumente\\Studium\\Master\\Vorlesungen\\SS24\\PMDS\\Codons Repo\\notebooks\\../scripts\\ml_helper.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=['sequence_length'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L채nge train_dataset: 2284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\insab\\OneDrive\\_Dokumente\\Studium\\Master\\Vorlesungen\\SS24\\PMDS\\Codons Repo\\notebooks\\../scripts\\ml_helper.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=['sequence_length'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L채nge test_dataset: 592\n"
     ]
    }
   ],
   "source": [
    "organism = \"E.Coli\"\n",
    "min_length = 100\n",
    "max_length = 500\n",
    "\n",
    "train_dataset = ml_helper.CodonDataset(organism, \"train\", min_length, max_length, one_hot_aa=False)\n",
    "print(f\"L채nge train_dataset: {len(train_dataset)}\")\n",
    "test_dataset = ml_helper.CodonDataset(organism, \"test\", min_length, max_length, one_hot_aa=False)\n",
    "print(f\"L채nge test_dataset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([12, 16, 18,  0, 19,  6, 13,  3,  2, 19, 15,  1, 10, 18,  7,  3, 19,  1,\n",
      "         0, 19,  3,  7, 19, 15,  9,  0,  9, 11,  3,  7,  6, 13, 13, 15, 12, 10,\n",
      "         7, 14, 15,  7, 15,  7, 11, 16, 16,  4, 10,  1, 10,  9,  0,  7, 13,  6,\n",
      "         5, 10, 15,  7,  7,  0,  9, 15,  9, 13,  7, 11, 14,  0, 15,  2, 10, 14,\n",
      "        14, 17,  6,  1,  3, 19,  2, 16, 19, 13,  5,  3, 18,  0, 10, 13, 14,  8,\n",
      "        12, 15,  9, 10,  3,  2, 19,  0, 18,  7, 10, 12, 19, 11,  7, 19,  2, 11,\n",
      "        11,  5,  1,  8,  0, 12,  0,  5,  6,  0, 10,  6, 11, 19,  0, 10,  7, 13,\n",
      "        19,  8,  5,  1, 11, 14, 15,  5, 10, 15,  7,  7,  5,  1,  5,  1, 19,  0,\n",
      "         9,  0,  1,  0, 10, 19,  2,  6, 14,  1, 19, 10, 10, 10,  3,  6, 14, 10,\n",
      "         7,  0, 10,  3, 10, 11, 10,  1,  6,  5, 12,  5, 10,  6, 10, 11, 11, 10,\n",
      "         5,  5, 15, 10,  7,  9, 16, 13,  9, 13, 19, 16,  8,  3,  5,  7,  6,  0,\n",
      "        10, 15, 12, 15,  3,  1, 19,  0, 19, 13,  2,  2,  7,  1,  9,  6,  5, 19,\n",
      "         3, 15, 14,  1,  3, 10, 18, 12,  1, 14,  1, 16, 14, 13, 19,  0,  7, 13,\n",
      "        19,  7, 16, 15,  2, 19, 13,  3,  7, 10, 12,  0,  6, 11, 10,  4,  7, 12,\n",
      "        16,  7, 15, 13,  0, 10,  1, 14,  6,  8,  9,  1, 10,  2, 16, 14,  7,  6,\n",
      "        10,  5,  0,  2,  7, 16,  9,  5,  0, 19,  5, 18,  5,  7,  0,  0, 16,  1,\n",
      "        13,  6, 10, 11, 10,  2,  7,  7,  6, 11, 10, 10, 19, 15,  5,  0,  2, 12,\n",
      "        16,  7,  6,  6, 10, 14,  0, 16, 10, 16, 14,  7,  5,  5, 19, 12, 19, 15,\n",
      "        17, 15,  1,  3, 19, 12, 19, 14, 10, 19,  6,  6,  1, 20, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21],\n",
      "       dtype=torch.int32), tensor([35., 39.,  9., 54., 51., 59.,  0., 57., 41., 49.,  7., 31.,  3.,  9.,\n",
      "        60., 57., 51., 29., 54., 50., 56., 61., 49., 44., 32., 55., 34., 42.,\n",
      "        56., 60., 59.,  1.,  1.,  4., 35., 19., 63., 23.,  5., 61.,  5., 61.,\n",
      "        42., 37., 37., 13., 19., 29., 19., 32., 52., 61.,  1., 58., 27., 16.,\n",
      "         5., 61., 63., 52., 33.,  4., 33.,  0., 60., 42., 21., 53., 45., 40.,\n",
      "        19., 22., 23., 15., 59., 31., 57., 51., 40., 36., 49.,  0., 27., 57.,\n",
      "         9., 55., 18.,  0., 23., 24., 35.,  7., 32., 16., 57., 40., 49., 53.,\n",
      "         8., 63., 19., 35., 49., 42., 61., 51., 40., 42., 43., 27., 31., 25.,\n",
      "        54., 35., 55., 26., 59., 55., 19., 59., 42., 51., 55.,  3., 63.,  0.,\n",
      "        50., 24., 26., 28., 42., 23.,  6., 26., 16.,  4., 60., 60., 27., 29.,\n",
      "        27., 31., 48., 52., 33., 53., 46., 54.,  3., 51., 40., 58., 23., 29.,\n",
      "        50.,  3., 19.,  3., 56., 58., 23., 17., 61., 54., 19., 56., 17., 42.,\n",
      "         3., 28., 59., 27., 35., 27., 19., 58., 19., 42., 42., 19., 26., 27.,\n",
      "         4., 17., 60., 33., 36.,  0., 33.,  1., 48., 37., 25., 56., 27., 61.,\n",
      "        58., 55.,  2.,  7., 35.,  5., 56., 28., 51., 55., 48.,  1., 40., 40.,\n",
      "        62., 29., 32., 59., 27., 49., 56.,  5., 23., 29., 56., 17.,  8., 35.,\n",
      "        29., 23., 29., 39., 23.,  0., 48., 53., 63.,  1., 48., 60., 38.,  7.,\n",
      "        40., 48.,  0., 56., 62., 19., 35., 54., 59., 42., 16., 12., 61., 35.,\n",
      "        39., 62., 45.,  1., 53., 19., 30., 23., 58., 24., 33., 29., 17., 41.,\n",
      "        37., 20., 60., 58., 19., 27., 53., 40., 61., 39., 33., 27., 55., 51.,\n",
      "        26.,  8., 27., 61., 55., 54., 36., 28.,  0., 58., 19., 42.,  3., 41.,\n",
      "        61., 60., 58., 42., 19., 16., 51., 44., 27., 53., 40., 35., 38., 61.,\n",
      "        58., 58., 19., 20., 53., 39., 17., 39., 21., 62., 26., 27., 51., 35.,\n",
      "        48.,  5., 15.,  7., 28., 56., 51., 35., 51., 23., 19., 48., 59., 59.,\n",
      "        47., 14., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64., 64., 64., 64.]))\n",
      "torch.Size([500])\n",
      "torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_dataset[0][0].shape)\n",
    "print(train_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the encoder-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_layers, num_heads):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(len(amino_acids), embed_dim, padding_idx=len(amino_acids)-1)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, len(codons))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_HEADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (emb): Embedding(22, 256, padding_idx=21)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=256, out_features=65, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EncoderClassifier(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    num_heads=NUM_HEADS\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "def print_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{total_params:,} total parameters.\")\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,282,625 total parameters.\n",
      "5,282,625 training parameters.\n"
     ]
    }
   ],
   "source": [
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(model, data_loader):\n",
    "  batch_data, batch_label = next(iter(data_loader))\n",
    "  print(f\"input dim: {batch_data.shape}\")\n",
    "  output = model(batch_data)\n",
    "  print(f\"output dim: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: torch.Size([32, 500])\n",
      "output dim: torch.Size([32, 500, 65])\n"
     ]
    }
   ],
   "source": [
    "test_forward_pass(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs, print_batches=0):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            input_data, labels = batch\n",
    "            output = model(input_data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_batches != 0 and batch_idx % print_batches == (print_batches-1):\n",
    "                batch_time =  round(time.time() - batch_start_time,2)\n",
    "                print(f'Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Time since last batch print: {batch_time} s')\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        epoch_time = round(time.time() - epoch_start_time,2)\n",
    "        epoch_loss = round(epoch_loss / len(train_loader),4)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Took {epoch_time} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluation methods to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accuracies = []\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_data, labels = batch\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_data)\n",
    "            outputs = outputs.view(-1, len(codons))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute custom metrics\n",
    "            accuracy = compute_accuracy(outputs.cpu(), labels.cpu())\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    # Compute average accuracy\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "    print(f'Average Loss: {avg_loss:.4f}')\n",
    "    print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    return outputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849,089 total parameters.\n",
      "849,089 training parameters.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EncoderClassifier(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    num_heads=NUM_HEADS\n",
    ").to(device)\n",
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Start Training -----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [32, 65], got [32, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----- Start Training -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m train_model(model, optimizer, criterion, EPOCHS, print_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[87], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, criterion, num_epochs, print_batches)\u001b[0m\n\u001b[0;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     18\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [32, 65], got [32, 500]"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHS = 5\n",
    "print(\"----- Start Training -----\")\n",
    "train_model(model, optimizer, criterion, EPOCHS, print_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 500])\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label = next(iter(train_loader))\n",
    "output = model(batch_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_model(model, criterion)\n",
      "Cell \u001b[1;32mIn[35], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, criterion)\u001b[0m\n\u001b[0;32m     16\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Compute custom metrics\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m compute_accuracy(outputs\u001b[38;5;241m.\u001b[39mcpu(), labels\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     20\u001b[0m         accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Compute average loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[1;34m(predictions, labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(predictions, labels):\n\u001b[1;32m----> 4\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(labels, predictions)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\insab\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
