{"cells":[{"cell_type":"markdown","metadata":{"id":"dvibukPbQ7Zg"},"source":["# Encoder-only Transformer Architektur"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20842,"status":"ok","timestamp":1715688662385,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"fXeUV5wCRtUz","outputId":"2bb0af2b-b2a5-4f6e-c9d6-addcb643a568"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8804,"status":"ok","timestamp":1715688671184,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"3yudZ4edSJEQ","outputId":"d4151a0c-b418-4144-d13b-0f4aef17b8a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: biopython in /home/mkuehn/.local/lib/python3.10/site-packages (1.83)\n","Requirement already satisfied: numpy in /home/mkuehn/.local/lib/python3.10/site-packages (from biopython) (1.26.4)\n"]}],"source":["!pip install biopython"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5081,"status":"ok","timestamp":1715688676256,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"ev_KN7VqQ7Zm"},"outputs":[],"source":["import sys\n","import random\n","import numpy as np\n","import pandas as pd\n","import ast\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch import Tensor\n","import time\n","import math\n","\n","sys.path.append('../scripts')\n","#sys.path.append('/content/drive/MyDrive/PMDS/Notebooks')\n","import ml_helper"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715688676256,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"9eRDLHmaS7Im"},"outputs":[],"source":["#data_path = '/content/drive/MyDrive/PMDS/Data'\n","data_path = '../data'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715688676256,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"0ayA8mz2Q7Zp"},"outputs":[],"source":["SEED = 42\n","def set_seed(SEED=SEED):\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","set_seed()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1715688676819,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"5hFAHJ8hRE8p","outputId":"6c9d739b-7b72-447c-878a-a0291901b15d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"1iTm60N3Q7Zr"},"source":["## Prepare Test and Training Data Loader"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715688680014,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"iZLLB2yhQ7Zs"},"outputs":[],"source":["amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', '*',\n","               '_']\n","\n","aminoacids_to_integer = dict((a, i) for i, a in enumerate(amino_acids))\n","integer_to_aminoacids = dict((i, a) for i, a in enumerate(amino_acids))\n","\n","codons = ['TTT', 'TTC', 'TTA', 'TTG', 'TCT', 'TCC', 'TCA', 'TCG', 'TAT', 'TAC', 'TAA', 'TAG', 'TGT', 'TGC', 'TGA',\n","          'TGG', 'CTT', 'CTC', 'CTA', 'CTG', 'CCT', 'CCC', 'CCA', 'CCG', 'CAT', 'CAC', 'CAA', 'CAG', 'CGT', 'CGC',\n","          'CGA', 'CGG', 'ATT', 'ATC', 'ATA', 'ATG', 'ACT', 'ACC', 'ACA', 'ACG', 'AAT', 'AAC', 'AAA', 'AAG', 'AGT',\n","          'AGC', 'AGA', 'AGG', 'GTT', 'GTC', 'GTA', 'GTG', 'GCT', 'GCC', 'GCA', 'GCG', 'GAT', 'GAC', 'GAA', 'GAG',\n","          'GGT', 'GGC', 'GGA', 'GGG', '___']\n","\n","codons_to_integer = dict((c, i) for i, c in enumerate(codons))\n","integer_to_codons = dict((i, c) for i, c in enumerate(codons))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14935,"status":"ok","timestamp":1715688695565,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"8w1wA5dDQ7Zt","outputId":"f67d1129-17d1-4a11-f6b0-83a4f973ba75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Länge train_dataset: 3561\n","Länge test_dataset: 864\n"]}],"source":["organism = \"E.Coli\"\n","min_length = None\n","max_length = 500\n","\n","SPEEDS_ADDED = False\n","\n","train_dataset = ml_helper.CodonDataset(organism, \"train\", min_length, max_length, add_speeds=SPEEDS_ADDED, cut_data=True, one_hot_aa=False, data_path=data_path, device=device)\n","print(f\"Länge train_dataset: {len(train_dataset)}\")\n","test_dataset = ml_helper.CodonDataset(organism, \"test\", min_length, max_length, add_speeds=SPEEDS_ADDED, cut_data=True, one_hot_aa=False, data_path=data_path, device=device)\n","print(f\"Länge test_dataset: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1715688695930,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"TDq7AeaFQ7Zv","outputId":"1b96196c-89a7-4031-b63e-6a9358f7aa09"},"outputs":[{"name":"stdout","output_type":"stream","text":["(tensor([12, 16, 18,  0, 19,  6, 13,  3,  2, 19, 15,  1, 10, 18,  7,  3, 19,  1,\n","         0, 19,  3,  7, 19, 15,  9,  0,  9, 11,  3,  7,  6, 13, 13, 15, 12, 10,\n","         7, 14, 15,  7, 15,  7, 11, 16, 16,  4, 10,  1, 10,  9,  0,  7, 13,  6,\n","         5, 10, 15,  7,  7,  0,  9, 15,  9, 13,  7, 11, 14,  0, 15,  2, 10, 14,\n","        14, 17,  6,  1,  3, 19,  2, 16, 19, 13,  5,  3, 18,  0, 10, 13, 14,  8,\n","        12, 15,  9, 10,  3,  2, 19,  0, 18,  7, 10, 12, 19, 11,  7, 19,  2, 11,\n","        11,  5,  1,  8,  0, 12,  0,  5,  6,  0, 10,  6, 11, 19,  0, 10,  7, 13,\n","        19,  8,  5,  1, 11, 14, 15,  5, 10, 15,  7,  7,  5,  1,  5,  1, 19,  0,\n","         9,  0,  1,  0, 10, 19,  2,  6, 14,  1, 19, 10, 10, 10,  3,  6, 14, 10,\n","         7,  0, 10,  3, 10, 11, 10,  1,  6,  5, 12,  5, 10,  6, 10, 11, 11, 10,\n","         5,  5, 15, 10,  7,  9, 16, 13,  9, 13, 19, 16,  8,  3,  5,  7,  6,  0,\n","        10, 15, 12, 15,  3,  1, 19,  0, 19, 13,  2,  2,  7,  1,  9,  6,  5, 19,\n","         3, 15, 14,  1,  3, 10, 18, 12,  1, 14,  1, 16, 14, 13, 19,  0,  7, 13,\n","        19,  7, 16, 15,  2, 19, 13,  3,  7, 10, 12,  0,  6, 11, 10,  4,  7, 12,\n","        16,  7, 15, 13,  0, 10,  1, 14,  6,  8,  9,  1, 10,  2, 16, 14,  7,  6,\n","        10,  5,  0,  2,  7, 16,  9,  5,  0, 19,  5, 18,  5,  7,  0,  0, 16,  1,\n","        13,  6, 10, 11, 10,  2,  7,  7,  6, 11, 10, 10, 19, 15,  5,  0,  2, 12,\n","        16,  7,  6,  6, 10, 14,  0, 16, 10, 16, 14,  7,  5,  5, 19, 12, 19, 15,\n","        17, 15,  1,  3, 19, 12, 19, 14, 10, 19,  6,  6,  1, 20, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n","        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21],\n","       device='cuda:0', dtype=torch.int32), tensor([35., 39.,  9., 54., 51., 59.,  0., 57., 41., 49.,  7., 31.,  3.,  9.,\n","        60., 57., 51., 29., 54., 50., 56., 61., 49., 44., 32., 55., 34., 42.,\n","        56., 60., 59.,  1.,  1.,  4., 35., 19., 63., 23.,  5., 61.,  5., 61.,\n","        42., 37., 37., 13., 19., 29., 19., 32., 52., 61.,  1., 58., 27., 16.,\n","         5., 61., 63., 52., 33.,  4., 33.,  0., 60., 42., 21., 53., 45., 40.,\n","        19., 22., 23., 15., 59., 31., 57., 51., 40., 36., 49.,  0., 27., 57.,\n","         9., 55., 18.,  0., 23., 24., 35.,  7., 32., 16., 57., 40., 49., 53.,\n","         8., 63., 19., 35., 49., 42., 61., 51., 40., 42., 43., 27., 31., 25.,\n","        54., 35., 55., 26., 59., 55., 19., 59., 42., 51., 55.,  3., 63.,  0.,\n","        50., 24., 26., 28., 42., 23.,  6., 26., 16.,  4., 60., 60., 27., 29.,\n","        27., 31., 48., 52., 33., 53., 46., 54.,  3., 51., 40., 58., 23., 29.,\n","        50.,  3., 19.,  3., 56., 58., 23., 17., 61., 54., 19., 56., 17., 42.,\n","         3., 28., 59., 27., 35., 27., 19., 58., 19., 42., 42., 19., 26., 27.,\n","         4., 17., 60., 33., 36.,  0., 33.,  1., 48., 37., 25., 56., 27., 61.,\n","        58., 55.,  2.,  7., 35.,  5., 56., 28., 51., 55., 48.,  1., 40., 40.,\n","        62., 29., 32., 59., 27., 49., 56.,  5., 23., 29., 56., 17.,  8., 35.,\n","        29., 23., 29., 39., 23.,  0., 48., 53., 63.,  1., 48., 60., 38.,  7.,\n","        40., 48.,  0., 56., 62., 19., 35., 54., 59., 42., 16., 12., 61., 35.,\n","        39., 62., 45.,  1., 53., 19., 30., 23., 58., 24., 33., 29., 17., 41.,\n","        37., 20., 60., 58., 19., 27., 53., 40., 61., 39., 33., 27., 55., 51.,\n","        26.,  8., 27., 61., 55., 54., 36., 28.,  0., 58., 19., 42.,  3., 41.,\n","        61., 60., 58., 42., 19., 16., 51., 44., 27., 53., 40., 35., 38., 61.,\n","        58., 58., 19., 20., 53., 39., 17., 39., 21., 62., 26., 27., 51., 35.,\n","        48.,  5., 15.,  7., 28., 56., 51., 35., 51., 23., 19., 48., 59., 59.,\n","        47., 14., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n","        64., 64., 64., 64., 64., 64., 64., 64., 64., 64.], device='cuda:0'))\n","torch.Size([500])\n","torch.Size([500])\n"]}],"source":["print(train_dataset[3])\n","print(train_dataset[3][0].shape)\n","print(train_dataset[3][1].shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715688695930,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"MQvx476IQ7Zw"},"outputs":[],"source":["BATCH_SIZE = 32\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"6UP5EBpZQ7Zx"},"source":["## Define the encoder-only model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Arguments:\n","            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1715688700989,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"DmAoTz7sQ7Zx"},"outputs":[],"source":["class EncoderClassifier(nn.Module):\n","    def __init__(self, embed_dim, num_layers, num_heads, dropout=0.2, pos_enc=False):\n","        super(EncoderClassifier, self).__init__()\n","\n","        emb_size = embed_dim\n","        if SPEEDS_ADDED:\n","            emb_size -= 1\n","        self.emb = nn.Embedding(len(amino_acids), emb_size, padding_idx=len(amino_acids)-1)\n","        self.pos_enc = pos_enc\n","        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n","\n","        self.encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,\n","            nhead=num_heads,\n","            batch_first=True\n","        )\n","        self.encoder = nn.TransformerEncoder(\n","            encoder_layer=self.encoder_layer,\n","            num_layers=num_layers,\n","        )\n","        self.linear = nn.Linear(embed_dim, len(codons))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = x.long()\n","        if SPEEDS_ADDED:\n","            x1 = self.emb(x[:, :, 0])\n","            x2 = x[:, :, 1].unsqueeze(-1)\n","            x = torch.cat((x1, x2), dim=-1)  # Concatenate along the feature dimension\n","        else:\n","            x = self.emb(x)\n","\n","        if self.pos_enc:\n","            x = x.transpose(0, 1)\n","            x = self.pos_encoder(x)  # Add positional encoding\n","            x = x.transpose(0, 1)\n","        x = self.encoder(x)\n","        x = self.dropout(x)\n","        out = self.linear(x)\n","        return out"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715688701504,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"2u2Dbb0rQ7Zy"},"outputs":[],"source":["EMBED_DIM = 256\n","NUM_ENCODER_LAYERS = 4\n","NUM_HEADS = 4\n","DROP_OUT = 0.2"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715688702540,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"33ksA3YLQ7Zy","outputId":"559e6f1f-4377-402d-fe71-a0adf4a7a70f"},"outputs":[{"name":"stdout","output_type":"stream","text":["EncoderClassifier(\n","  (emb): Embedding(22, 256, padding_idx=21)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (encoder_layer): TransformerEncoderLayer(\n","    (self_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","    )\n","    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (dropout1): Dropout(p=0.1, inplace=False)\n","    (dropout2): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=256, out_features=65, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}],"source":["model = EncoderClassifier(\n","    embed_dim=EMBED_DIM,\n","    num_layers=NUM_ENCODER_LAYERS,\n","    num_heads=NUM_HEADS,\n","    dropout=DROP_OUT,\n","    pos_enc=True\n",").to(device)\n","print(model)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715688702799,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"DLRNk1mnQ7Zz"},"outputs":[],"source":["# Total parameters and trainable parameters.\n","def print_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    print(f\"{total_params:,} total parameters.\")\n","    total_trainable_params = sum(\n","        p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"{total_trainable_params:,} training parameters.\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715688703433,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"Ua_2VN3NQ7Zz","outputId":"472c69d7-94cb-40cd-bd7e-8e1e75c00415"},"outputs":[{"name":"stdout","output_type":"stream","text":["6,597,697 total parameters.\n","6,597,697 training parameters.\n"]}],"source":["print_parameters(model)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715688704233,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"6SEl0FSxQ7Zz"},"outputs":[],"source":["def test_forward_pass(model, data_loader):\n","  batch_data, batch_label = next(iter(data_loader))\n","  print(f\"input dim: {batch_data.shape}\")\n","  output = model(batch_data)\n","  print(f\"output dim: {output.shape}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1152,"status":"ok","timestamp":1715688705880,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"X9OgGtB6Q7Z0","outputId":"52e399fa-b185-4c52-c314-91c772f4e98f"},"outputs":[{"name":"stdout","output_type":"stream","text":["input dim: torch.Size([32, 500])\n","output dim: torch.Size([32, 500, 65])\n"]}],"source":["test_forward_pass(model, train_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Define the evaluation methods to calculate metrics"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["def count_correct_predictions(predictions, labels):\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Find indices where labels are not equal to the padding value\n","    non_padding_indices = labels != codons_to_integer['___']\n","\n","    # Filter out predictions and labels where the label is not padding\n","    filtered_predictions = predictions[non_padding_indices]\n","    filtered_labels = labels[non_padding_indices]\n","\n","    codon_num = filtered_labels.shape[0]\n","    correct_codons = (filtered_predictions == filtered_labels).sum().item()\n","    return codon_num, correct_codons"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[],"source":["def evaluate_model(model, criterion, print_scores=True, loss_without_pad=False):\n","    model.eval()  # Set the model to evaluation mode\n","\n","    total_loss = 0.0\n","\n","    with torch.no_grad():\n","        codon_num = 0\n","        correct_codon_num = 0\n","        for batch_idx, batch in enumerate(test_loader):\n","             # Forward pass\n","            input_data, labels = batch\n","\n","            output = model(input_data)  # (batch_size, seq_len, num_classes)\n","            output = output.view(-1, len(codons)) # (batch_size * seq_len, num_classes)\n","\n","            labels = labels.view(-1).long() # (batch_size, seq_len) -> (batch_size * seq_len)\n","\n","            # Calculate loss\n","            loss = criterion(output, labels)\n","\n","            # Compute total loss\n","            total_loss += loss.item()\n","\n","            # Count codons and correct codon predictions\n","            codon_num_batch, correct_codons_batch = count_correct_predictions(output.cpu(), labels.cpu())\n","            codon_num += codon_num_batch\n","            correct_codon_num += correct_codons_batch\n","\n","    # Compute average loss\n","    avg_loss = total_loss / len(test_loader)\n","\n","    # Compute accuracy\n","    accuracy = round(correct_codon_num / codon_num, 4)\n","\n","    if print_scores:\n","        print(f'Average Batch Loss: {avg_loss:.4f}')\n","        print(f'Accuracy: {accuracy:.4f}')\n","\n","    return avg_loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"6DIXDxybQ7Z0"},"source":["## Define the training methods"]},{"cell_type":"code","execution_count":204,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1715688705880,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"B8_PCftyQ7Z0"},"outputs":[],"source":["def train_model(model, num_epochs, loss_ignore_pad=True, learning_rate=0.0005, validation_stop=True, validation_stop_area=5, print_batches=0, print_epochs=True):\n","    criterion = torch.nn.CrossEntropyLoss()\n","    if loss_ignore_pad:\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=codons_to_integer['___'])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    \n","    start_time = time.time()\n","    last_loss = None\n","    saved_accuracies = []\n","    for epoch in range(num_epochs):\n","        set_seed(epoch)\n","        model.train()\n","\n","        epoch_start_time = time.time()\n","        batch_start_time = time.time()\n","        epoch_loss = 0\n","        for batch_idx, batch in enumerate(train_loader):\n","            # Clear gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            input_data, labels = batch\n","\n","            output = model(input_data)  # (batch_size, seq_len, num_classes)\n","            output = output.view(-1, len(codons)) # (batch_size * seq_len, num_classes)\n","\n","            labels = labels.view(-1).long() # (batch_size, seq_len) -> (batch_size * seq_len)\n","\n","            # Calculate loss\n","            loss = criterion(output, labels)\n","            epoch_loss += loss.item()\n","\n","            # Backward pass\n","            loss.backward()\n","\n","            # Update model parameters\n","            optimizer.step()\n","\n","            if print_batches != 0 and batch_idx % print_batches == (print_batches-1):\n","                batch_time =  round(time.time() - batch_start_time,2)\n","                print(f'Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Time since last batch print: {batch_time} s')\n","                batch_start_time = time.time()\n","        \n","        epoch_loss = round(epoch_loss / len(train_loader),4)\n","        last_loss = epoch_loss\n","        \n","        avg_eval_loss, accuracy = evaluate_model(model, criterion, print_scores=False)\n","        \n","        epoch_time = round(time.time() - epoch_start_time,2)\n","        if print_epochs:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Eval Accuracy: {accuracy}, Took {epoch_time} s')\n","         \n","        if validation_stop:\n","            if len(saved_accuracies) == validation_stop_area:\n","                # compare accuracy to average of saved_accuracies\n","                # if accuracy is lower: stop early\n","                if accuracy < np.average(np.array(saved_accuracies)):\n","                    print(f'Stopped early after epoch {epoch+1} as validation accuracy was lower than average of the last {validation_stop_area} accuracies.')\n","                    break\n","                saved_accuracies.pop(0)\n","            saved_accuracies.append(accuracy)      \n","               \n","    total_time = round(time.time() - start_time,2)\n","    print(f'Last Loss: {last_loss}, Last Eval Accuracy: {accuracy}, Took {total_time} s')\n","    return last_loss"]},{"cell_type":"markdown","metadata":{"id":"w60mTX5nQ7Z1"},"source":["## Training the model"]},{"cell_type":"code","execution_count":198,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715688892531,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"Vr8Ge8QYQ7Z2"},"outputs":[{"name":"stdout","output_type":"stream","text":["6,597,697 total parameters.\n","6,597,697 training parameters.\n"]}],"source":["set_seed()\n","EMBED_DIM = 256\n","NUM_ENCODER_LAYERS = 4\n","NUM_HEADS = 4\n","DROPOUT = 0.3\n","\n","model = EncoderClassifier(\n","    embed_dim=EMBED_DIM,\n","    num_layers=NUM_ENCODER_LAYERS,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT,\n","    pos_enc=False\n",").to(device)\n","print_parameters(model)"]},{"cell_type":"code","execution_count":199,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1715688894105,"user":{"displayName":"Insa Belter","userId":"09408038895575287386"},"user_tz":-120},"id":"8G-AucPcQ7Z2","outputId":"8483db98-2a68-47ee-eaa2-b7d3c55e90f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["----- Start Training -----\n","Epoch [1/50], Loss: 1.1839, Eval Accuracy: 0.5099, Took 3.86 s\n","Epoch [2/50], Loss: 1.0624, Eval Accuracy: 0.5194, Took 3.86 s\n","Epoch [3/50], Loss: 1.0521, Eval Accuracy: 0.5162, Took 3.86 s\n","Epoch [4/50], Loss: 1.0446, Eval Accuracy: 0.5169, Took 3.86 s\n","Epoch [5/50], Loss: 1.0405, Eval Accuracy: 0.5233, Took 3.84 s\n","Epoch [6/50], Loss: 1.0367, Eval Accuracy: 0.5196, Took 3.83 s\n","Epoch [7/50], Loss: 1.0354, Eval Accuracy: 0.5237, Took 3.84 s\n","Epoch [8/50], Loss: 1.0343, Eval Accuracy: 0.5234, Took 3.84 s\n","Epoch [9/50], Loss: 1.0322, Eval Accuracy: 0.5253, Took 3.84 s\n","Epoch [10/50], Loss: 1.0306, Eval Accuracy: 0.5245, Took 3.84 s\n","Epoch [11/50], Loss: 1.0307, Eval Accuracy: 0.5244, Took 3.84 s\n","Epoch [12/50], Loss: 1.0303, Eval Accuracy: 0.5248, Took 3.84 s\n","Epoch [13/50], Loss: 1.0296, Eval Accuracy: 0.5246, Took 3.84 s\n","Epoch [14/50], Loss: 1.0294, Eval Accuracy: 0.5251, Took 3.84 s\n","Epoch [15/50], Loss: 1.0284, Eval Accuracy: 0.5248, Took 3.84 s\n","Epoch [16/50], Loss: 1.028, Eval Accuracy: 0.5248, Took 3.84 s\n","Epoch [17/50], Loss: 1.0286, Eval Accuracy: 0.526, Took 3.84 s\n","Epoch [18/50], Loss: 1.0279, Eval Accuracy: 0.5234, Took 3.84 s\n","Stopped early after epoch 18 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0279, Took 69.19 s\n"]},{"data":{"text/plain":["1.0279"]},"execution_count":199,"metadata":{},"output_type":"execute_result"}],"source":["EPOCHS = 50\n","print(\"----- Start Training -----\")\n","train_model(model, EPOCHS)"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved as 20240528165813_encoder_256em_4l_4h_03dr_10ep.pt\n"]}],"source":["ml_helper.save_model(model, f'encoder_256em_4l_4h_03dr_10ep', organism)"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter tuning"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[],"source":["def train_parameter_model(embed_dim, num_encoder_layers, num_heads, dropout, pos_enc, num_epochs):\n","    set_seed()\n","    \n","    model = EncoderClassifier(\n","        embed_dim=embed_dim,\n","        num_layers=num_encoder_layers,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        pos_enc=pos_enc\n","    ).to(device)\n","    #print_parameters(model)\n","\n","    print(f\"----- Start Training: {embed_dim} emb, {num_encoder_layers} layers, {num_heads} heads, {dropout} dropout, positional encoding: {pos_enc}, {num_epochs} epochs -----\")\n","    last_loss = train_model(model, num_epochs, print_epochs=False)\n","\n","    saved = False\n","    if last_loss >= 1.5:\n","        print(f\"Did not save following model as loss was too high:\")\n","        print(f'encoder_{embed_dim}em_{num_encoder_layers}l_{num_heads}h_{str(dropout).replace(\".\",\"\")}dr_{num_epochs}ep{\"_posenc\" if pos_enc else \"\"}')\n","    else:\n","        saved = True\n","        ml_helper.save_model(model, f'encoder_{embed_dim}em_{num_encoder_layers}l_{num_heads}h_{str(dropout).replace(\".\",\"\")}dr_{num_epochs}ep{\"_posenc\" if pos_enc else \"\"}', organism)\n","    return saved"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[],"source":["def hyper_parameter_training(embed_dims, num_encoder_layers, num_heads, dropouts, pos_enc, epochs=30):\n","    not_saved = []\n","    for EMBED_DIM in embed_dims:\n","        for NUM_ENCODER_LAYERS in num_encoder_layers:\n","            for NUM_HEADS in num_heads:\n","                for DROPOUT in dropouts:\n","                    for POS_ENC in pos_enc:\n","                        saved = train_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, epochs)\n","                        if not saved:\n","                            not_saved.append(f'encoder_{EMBED_DIM}em_{NUM_ENCODER_LAYERS}l_{NUM_HEADS}h_{str(DROPOUT).replace(\".\",\"\")}dr_{epochs}ep{\"_posenc\" if POS_ENC else \"\"}')\n","    print(\"------------\")\n","    print(\"Not saved as loss too high:\")\n","    print(not_saved)"]},{"cell_type":"markdown","metadata":{},"source":["### E.Coli"]},{"cell_type":"markdown","metadata":{},"source":["#### Dropout"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----- Start Training: 256 emb, 4 layers, 4 heads, 0.1 dropout, positional encoding: False, 30 epochs -----\n","Stopped early after epoch 13 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0294, Last Eval Accuracy: 0.5242, Took 49.54 s\n","Model saved as 20240528170108_encoder_256em_4l_4h_01dr_30ep.pt\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.2 dropout, positional encoding: False, 30 epochs -----\n","Stopped early after epoch 18 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0275, Last Eval Accuracy: 0.5238, Took 68.8 s\n","Model saved as 20240528170217_encoder_256em_4l_4h_02dr_30ep.pt\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.3 dropout, positional encoding: False, 30 epochs -----\n","Stopped early after epoch 18 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0279, Last Eval Accuracy: 0.5234, Took 69.01 s\n","Model saved as 20240528170326_encoder_256em_4l_4h_03dr_30ep.pt\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.4 dropout, positional encoding: False, 30 epochs -----\n","Stopped early after epoch 15 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0288, Last Eval Accuracy: 0.5246, Took 57.72 s\n","Model saved as 20240528170424_encoder_256em_4l_4h_04dr_30ep.pt\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.5 dropout, positional encoding: False, 30 epochs -----\n","Stopped early after epoch 13 as validation accuracy was lower than average of the last 5 accuracies.\n","Last Loss: 1.0306, Last Eval Accuracy: 0.5244, Took 49.89 s\n","Model saved as 20240528170514_encoder_256em_4l_4h_05dr_30ep.pt\n","------------\n","Not saved as loss too high:\n","[]\n"]}],"source":["EMBED_DIM = [256]\n","NUM_ENCODER_LAYERS = [4]\n","NUM_HEADS = [4]\n","dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]\n","POS_ENC = [False]\n","hyper_parameter_training(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, dropouts, POS_ENC)"]},{"cell_type":"markdown","metadata":{},"source":["#### Positional Encoding"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6,597,697 total parameters.\n","6,597,697 training parameters.\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.3 dropout, positional encoding: True, 10 epochs -----\n","Last Loss: 0.5595, Took 35.61 s\n","Model saved as 20240527164446_encoder_256em_4l_4h_03dr_10ep_posenc.pt\n","6,597,697 total parameters.\n","6,597,697 training parameters.\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 0.5604, Took 35.48 s\n","Model saved as 20240527164522_encoder_256em_4l_4h_03dr_10ep.pt\n","------------\n","Not saved as loss too high:\n","[]\n"]}],"source":["EMBED_DIM = [256]\n","NUM_ENCODER_LAYERS = [4]\n","NUM_HEADS = [4]\n","DROPOUTS = [0.3]\n","pos_enc = [True, False]\n","hyper_parameter_training(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUTS, pos_enc)"]},{"cell_type":"markdown","metadata":{},"source":["#### Embedding Dimension"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embed_dims = [32, 64, 128, 256, 512, 1028]\n","NUM_ENCODER_LAYERS = [4]\n","NUM_HEADS = [4]\n","DROPOUTS = [0.3]\n","POS_ENC = [False]\n","hyper_parameter_training(embed_dims, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUTS, POS_ENC)"]},{"cell_type":"markdown","metadata":{},"source":["#### Number Encoder Layers and Heads"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EMBED_DIM = [256]\n","num_encoder_layers = [2, 4, 8, 16]\n","num_heads = [2, 4, 8, 16]\n","DROPOUTS = [0.3]\n","POS_ENC = [False]\n","hyper_parameter_training(EMBED_DIM, num_encoder_layers, num_heads, DROPOUTS, POS_ENC)"]},{"cell_type":"markdown","metadata":{},"source":["## Drosophila.Melanogaster"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Länge train_dataset: 33071\n","Länge test_dataset: 8100\n"]}],"source":["organism = \"Drosophila.Melanogaster\"\n","min_length = None\n","max_length = 500\n","\n","train_dataset = ml_helper.CodonDataset(organism, \"train\", min_length, max_length, cut_data=True, one_hot_aa=False, data_path=data_path, device=device)\n","print(f\"Länge train_dataset: {len(train_dataset)}\")\n","test_dataset = ml_helper.CodonDataset(organism, \"test\", min_length, max_length, cut_data=True, one_hot_aa=False, data_path=data_path, device=device)\n","print(f\"Länge test_dataset: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embed_dims = [256, 512]\n","num_encoder_layers = [4, 8]\n","num_heads = [4, 8]\n","DROPOUTS = [0.3]\n","POS_ENC = [False]\n","hyper_parameter_training(embed_dims, num_encoder_layers, num_heads, DROPOUTS, POS_ENC)"]},{"cell_type":"markdown","metadata":{},"source":["## Homo.Sapiens"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Länge train_dataset: 140902\n","Länge test_dataset: 35210\n","CPU times: user 1min 41s, sys: 508 ms, total: 1min 42s\n","Wall time: 1min 42s\n"]}],"source":["%%time\n","\n","organism = \"Homo.Sapiens\"\n","min_length = None\n","max_length = 500\n","\n","train_dataset = ml_helper.CodonDataset(organism, \"train\", min_length, max_length, cut_data=True, one_hot_aa=False, filter_x=True, data_path=data_path, device=device)\n","print(f\"Länge train_dataset: {len(train_dataset)}\")\n","test_dataset = ml_helper.CodonDataset(organism, \"test\", min_length, max_length, cut_data=True, one_hot_aa=False, filter_x=True, data_path=data_path, device=device)\n","print(f\"Länge test_dataset: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6,597,697 total parameters.\n","6,597,697 training parameters.\n","----- Start Training: 256 emb, 4 layers, 4 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 0.5604, Took 35.08 s\n","Model saved as 20240527185016_encoder_256em_4l_4h_03dr_10ep.pt\n","6,597,697 total parameters.\n","6,597,697 training parameters.\n","----- Start Training: 256 emb, 4 layers, 8 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 0.5604, Took 40.84 s\n","Model saved as 20240527185057_encoder_256em_4l_8h_03dr_10ep.pt\n","9,227,841 total parameters.\n","9,227,841 training parameters.\n","----- Start Training: 256 emb, 6 layers, 4 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 2.8327, Took 52.7 s\n","Did not save following model as loss was too high:\n","encoder_256em_6l_4h_03dr_10ep\n","9,227,841 total parameters.\n","9,227,841 training parameters.\n","----- Start Training: 256 emb, 6 layers, 8 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 2.1347, Took 61.23 s\n","Did not save following model as loss was too high:\n","encoder_256em_6l_8h_03dr_10ep\n","15,806,529 total parameters.\n","15,806,529 training parameters.\n","----- Start Training: 512 emb, 4 layers, 4 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 0.5612, Took 65.59 s\n","Model saved as 20240527185357_encoder_512em_4l_4h_03dr_10ep.pt\n","15,806,529 total parameters.\n","15,806,529 training parameters.\n","----- Start Training: 512 emb, 4 layers, 8 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 0.5607, Took 67.21 s\n","Model saved as 20240527185504_encoder_512em_4l_8h_03dr_10ep.pt\n","22,111,297 total parameters.\n","22,111,297 training parameters.\n","----- Start Training: 512 emb, 6 layers, 4 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 2.8338, Took 98.44 s\n","Did not save following model as loss was too high:\n","encoder_512em_6l_4h_03dr_10ep\n","22,111,297 total parameters.\n","22,111,297 training parameters.\n","----- Start Training: 512 emb, 6 layers, 8 heads, 0.3 dropout, positional encoding: False, 10 epochs -----\n","Last Loss: 2.8338, Took 101.08 s\n","Did not save following model as loss was too high:\n","encoder_512em_6l_8h_03dr_10ep\n","------------\n","Not saved as loss too high:\n","['encoder_256em_6l_4h_03dr_10ep', 'encoder_256em_6l_8h_03dr_10ep', 'encoder_512em_6l_4h_03dr_10ep', 'encoder_512em_6l_8h_03dr_10ep']\n"]}],"source":["embed_dims = [256, 512]\n","num_encoder_layers = [4, 6]\n","num_heads = [4, 8]\n","DROPOUTS = [0.3]\n","POS_ENC = [False]\n","hyper_parameter_training(embed_dims, num_encoder_layers, num_heads, DROPOUTS, POS_ENC)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
