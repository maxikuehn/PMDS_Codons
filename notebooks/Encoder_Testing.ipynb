{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Codons using the trained Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "import time\n",
    "import math\n",
    "\n",
    "sys.path.append('../scripts')\n",
    "import ml_helper\n",
    "import Classifier as Classifier\n",
    "import Baseline_classifiers as bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', '*',\n",
    "               '_']\n",
    "\n",
    "aminoacids_to_integer = dict((a, i) for i, a in enumerate(amino_acids))\n",
    "integer_to_aminoacids = dict((i, a) for i, a in enumerate(amino_acids))\n",
    "\n",
    "codons = ['TTT', 'TTC', 'TTA', 'TTG', 'TCT', 'TCC', 'TCA', 'TCG', 'TAT', 'TAC', 'TAA', 'TAG', 'TGT', 'TGC', 'TGA',\n",
    "          'TGG', 'CTT', 'CTC', 'CTA', 'CTG', 'CCT', 'CCC', 'CCA', 'CCG', 'CAT', 'CAC', 'CAA', 'CAG', 'CGT', 'CGC',\n",
    "          'CGA', 'CGG', 'ATT', 'ATC', 'ATA', 'ATG', 'ACT', 'ACC', 'ACA', 'ACG', 'AAT', 'AAC', 'AAA', 'AAG', 'AGT',\n",
    "          'AGC', 'AGA', 'AGG', 'GTT', 'GTC', 'GTA', 'GTG', 'GCT', 'GCC', 'GCA', 'GCG', 'GAT', 'GAC', 'GAA', 'GAG',\n",
    "          'GGT', 'GGC', 'GGA', 'GGG', '___']\n",
    "\n",
    "codons_to_integer = dict((c, i) for i, c in enumerate(codons))\n",
    "integer_to_codons = dict((i, c) for i, c in enumerate(codons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>translation</th>\n",
       "      <th>seguid</th>\n",
       "      <th>codons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lcl|U00096.3_cds_AAC73113.1_2</td>\n",
       "      <td>lcl|U00096.3_cds_AAC73113.1_2 [gene=thrA] [loc...</td>\n",
       "      <td>(A, T, G, C, G, A, G, T, G, T, T, G, A, A, G, ...</td>\n",
       "      <td>(M, R, V, L, K, F, G, G, T, S, V, A, N, A, E, ...</td>\n",
       "      <td>/p+3Jdgat4Fq0w2rqqay4xg8Bs4</td>\n",
       "      <td>[ATG, CGA, GTG, TTG, AAG, TTC, GGC, GGT, ACA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lcl|U00096.3_cds_AAC73117.1_6</td>\n",
       "      <td>lcl|U00096.3_cds_AAC73117.1_6 [gene=yaaA] [loc...</td>\n",
       "      <td>(A, T, G, C, T, G, A, T, T, C, T, T, A, T, T, ...</td>\n",
       "      <td>(M, L, I, L, I, S, P, A, K, T, L, D, Y, Q, S, ...</td>\n",
       "      <td>vJJ0yR31YORqwI12U79SgItYU3U</td>\n",
       "      <td>[ATG, CTG, ATT, CTT, ATT, TCA, CCT, GCG, AAA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lcl|U00096.3_cds_AAC73124.1_13</td>\n",
       "      <td>lcl|U00096.3_cds_AAC73124.1_13 [gene=yaaI] [lo...</td>\n",
       "      <td>(A, T, G, A, A, A, T, C, C, G, T, T, T, T, T, ...</td>\n",
       "      <td>(M, K, S, V, F, T, I, S, A, S, L, A, I, S, L, ...</td>\n",
       "      <td>GT2zzYZoFncaOMVxs4CEcLaePdc</td>\n",
       "      <td>[ATG, AAA, TCC, GTT, TTT, ACG, ATT, TCC, GCC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lcl|U00096.3_cds_AAT48122.1_18</td>\n",
       "      <td>lcl|U00096.3_cds_AAT48122.1_18 [gene=hokC] [lo...</td>\n",
       "      <td>(A, T, G, A, A, G, C, A, G, C, A, T, A, A, G, ...</td>\n",
       "      <td>(M, K, Q, H, K, A, M, I, V, A, L, I, V, I, C, ...</td>\n",
       "      <td>yfUY1Sxn8BgBfdGY1FQnaroApNY</td>\n",
       "      <td>[ATG, AAG, CAG, CAT, AAG, GCG, ATG, ATT, GTC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lcl|U00096.3_cds_AAC73135.1_24</td>\n",
       "      <td>lcl|U00096.3_cds_AAC73135.1_24 [gene=yaaY] [lo...</td>\n",
       "      <td>(A, T, G, T, G, C, C, G, G, C, A, C, T, C, G, ...</td>\n",
       "      <td>(M, C, R, H, S, L, R, S, D, G, A, G, F, Y, Q, ...</td>\n",
       "      <td>m/3aWNuEiWlqAe7cvTcnrZ58efA</td>\n",
       "      <td>[ATG, TGC, CGG, CAC, TCG, TTA, CGT, AGT, GAT, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "1    lcl|U00096.3_cds_AAC73113.1_2   \n",
       "5    lcl|U00096.3_cds_AAC73117.1_6   \n",
       "12  lcl|U00096.3_cds_AAC73124.1_13   \n",
       "17  lcl|U00096.3_cds_AAT48122.1_18   \n",
       "23  lcl|U00096.3_cds_AAC73135.1_24   \n",
       "\n",
       "                                          description  \\\n",
       "1   lcl|U00096.3_cds_AAC73113.1_2 [gene=thrA] [loc...   \n",
       "5   lcl|U00096.3_cds_AAC73117.1_6 [gene=yaaA] [loc...   \n",
       "12  lcl|U00096.3_cds_AAC73124.1_13 [gene=yaaI] [lo...   \n",
       "17  lcl|U00096.3_cds_AAT48122.1_18 [gene=hokC] [lo...   \n",
       "23  lcl|U00096.3_cds_AAC73135.1_24 [gene=yaaY] [lo...   \n",
       "\n",
       "                                             sequence  \\\n",
       "1   (A, T, G, C, G, A, G, T, G, T, T, G, A, A, G, ...   \n",
       "5   (A, T, G, C, T, G, A, T, T, C, T, T, A, T, T, ...   \n",
       "12  (A, T, G, A, A, A, T, C, C, G, T, T, T, T, T, ...   \n",
       "17  (A, T, G, A, A, G, C, A, G, C, A, T, A, A, G, ...   \n",
       "23  (A, T, G, T, G, C, C, G, G, C, A, C, T, C, G, ...   \n",
       "\n",
       "                                          translation  \\\n",
       "1   (M, R, V, L, K, F, G, G, T, S, V, A, N, A, E, ...   \n",
       "5   (M, L, I, L, I, S, P, A, K, T, L, D, Y, Q, S, ...   \n",
       "12  (M, K, S, V, F, T, I, S, A, S, L, A, I, S, L, ...   \n",
       "17  (M, K, Q, H, K, A, M, I, V, A, L, I, V, I, C, ...   \n",
       "23  (M, C, R, H, S, L, R, S, D, G, A, G, F, Y, Q, ...   \n",
       "\n",
       "                         seguid  \\\n",
       "1   /p+3Jdgat4Fq0w2rqqay4xg8Bs4   \n",
       "5   vJJ0yR31YORqwI12U79SgItYU3U   \n",
       "12  GT2zzYZoFncaOMVxs4CEcLaePdc   \n",
       "17  yfUY1Sxn8BgBfdGY1FQnaroApNY   \n",
       "23  m/3aWNuEiWlqAe7cvTcnrZ58efA   \n",
       "\n",
       "                                               codons  \n",
       "1   [ATG, CGA, GTG, TTG, AAG, TTC, GGC, GGT, ACA, ...  \n",
       "5   [ATG, CTG, ATT, CTT, ATT, TCA, CCT, GCG, AAA, ...  \n",
       "12  [ATG, AAA, TCC, GTT, TTT, ACG, ATT, TCC, GCC, ...  \n",
       "17  [ATG, AAG, CAG, CAT, AAG, GCG, ATG, ATT, GTC, ...  \n",
       "23  [ATG, TGC, CGG, CAC, TCG, TTA, CGT, AGT, GAT, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "organism = \"E.Coli\"\n",
    "\n",
    "def group_codons(sequence):\n",
    "    return [''.join(sequence[i:i+3]) for i in range(0, len(sequence), 3)]\n",
    "\n",
    "df = pd.read_pickle(f\"../data/{organism}/cleanedData_test.pkl\")\n",
    "usage_biases = pd.read_pickle(f\"../data/{organism}/usageBias.pkl\")\n",
    "df['codons'] = df['sequence'].apply(group_codons)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = None\n",
    "max_length = None\n",
    "\n",
    "df = ml_helper.filter_sequence_length(df, min_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS_ADDED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_layers, num_heads, dropout=0.2, pos_enc=False):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "\n",
    "        emb_size = embed_dim\n",
    "        if SPEEDS_ADDED:\n",
    "            emb_size -= 1\n",
    "        self.emb = nn.Embedding(len(amino_acids), emb_size, padding_idx=len(amino_acids)-1)\n",
    "        self.pos_enc = pos_enc\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, len(codons))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        if SPEEDS_ADDED:\n",
    "            x1 = self.emb(x[:, :, 0])\n",
    "            x2 = x[:, :, 1].unsqueeze(-1)\n",
    "            x = torch.cat((x1, x2), dim=-1)  # Concatenate along the feature dimension\n",
    "        else:\n",
    "            x = self.emb(x)\n",
    "\n",
    "        if self.pos_enc:\n",
    "            x = self.pos_encoder(x)  # Add positional encoding\n",
    "        x = self.encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240522184144_encoder_256em_4l_4h_02dr_10ep_speeds.pt\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = EncoderClassifier(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    pos_enc=False\n",
    ").to(device)\n",
    "model = ml_helper.load_model('encoder_256em_4l_4h_02dr_10ep_speeds', organism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for max_length cutting and putting together again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequence(aa_sequence, max_length):\n",
    "    aa_sequences = []\n",
    "    bit_map = \"\" # 1 if sequence is cut, 0 if not\n",
    "    if aa_sequence.shape[0] <= max_length:\n",
    "        aa_sequences = [aa_sequence]\n",
    "        bit_map = \"0\"\n",
    "    elif aa_sequence.shape[0] > max_length:\n",
    "        aa_splits = ml_helper._split_tensor(aa_sequence, max_length)\n",
    "        aa_sequences = aa_splits\n",
    "        bit_map = \"1\" * (len(aa_splits) - 1) + \"0\"\n",
    "    return aa_sequences, bit_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_sequences(sequences, cut_bit_map):\n",
    "    new_sequences = []\n",
    "    new_sequence = None\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        if new_sequence is None:\n",
    "            new_sequence = sequence\n",
    "        elif new_sequence is not None:\n",
    "            if type(new_sequence) == torch.Tensor:\n",
    "                new_sequence = torch.cat((new_sequence, sequence))\n",
    "            elif type(new_sequence) == list:\n",
    "                new_sequence += sequence\n",
    "\n",
    "        if cut_bit_map[i] == \"0\":\n",
    "            new_sequences.append(new_sequence)\n",
    "            new_sequence = None\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(sequence, padding_value):\n",
    "    return sequence[sequence != padding_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_pos = 'right'\n",
    "def prepare_aa_sequence(aa_sequence):\n",
    "    max_length = 500\n",
    "    non_cut_aa_sequence = ml_helper.aa_to_int_tensor(aa_sequence, device)\n",
    "    aa_sequences, bit_map = cut_sequence(non_cut_aa_sequence, max_length)\n",
    "    for i, aa_sequence in enumerate(aa_sequences):\n",
    "        aa_sequences[i] = ml_helper.pad_tensor(aa_sequence, max_length, aminoacids_to_integer['_'], padding_pos)\n",
    "        if SPEEDS_ADDED:\n",
    "            aa_sequences[i] = ml_helper.add_speed_dimension(aa_sequences[i], device)\n",
    "    return aa_sequences, bit_map, non_cut_aa_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (pad, convert to tensor)\n",
    "prepared_amino_seq = []\n",
    "cut_bit_map = \"\"\n",
    "non_cut_aa_sequences = []\n",
    "i = 0\n",
    "for seq in df['translation']:\n",
    "    aa_sequences, bit_map, non_cut_aa_sequence = prepare_aa_sequence(seq)\n",
    "    prepared_amino_seq += aa_sequences\n",
    "    cut_bit_map += bit_map\n",
    "    non_cut_aa_sequences.append(non_cut_aa_sequence)\n",
    "# create data_loader for batched throughput\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(prepared_amino_seq, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPEEDS_ADDED:\n",
    "    for i, aa_sequence in enumerate(prepared_amino_seq):\n",
    "        prepared_amino_seq[i] = aa_sequence[:, 0].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sequences = rebuild_sequences(prepared_amino_seq, cut_bit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rebuild_sequence in enumerate(re_sequences):\n",
    "    re_sequences[i] = remove_padding(rebuild_sequence, aminoacids_to_integer[\"_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_lists_equal(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for tensor1, tensor2 in zip(list1, list2):\n",
    "        if tensor1.shape[0] != tensor2.shape[0]:\n",
    "            return False\n",
    "        if not torch.allclose(tensor1, tensor2):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_lists_equal(non_cut_aa_sequences, re_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the codon prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_codons(model, aa_sequence_list):\n",
    "    # Prepare data (pad, convert to tensor)\n",
    "    prepared_amino_seq = []\n",
    "    cut_bit_map = \"\"\n",
    "    for seq in aa_sequence_list:\n",
    "        aa_sequences, bit_map, _ = prepare_aa_sequence(seq)\n",
    "        prepared_amino_seq += aa_sequences\n",
    "        cut_bit_map += bit_map\n",
    "\n",
    "    # create data_loader for batched throughput\n",
    "    batch_size = 32\n",
    "    data_loader = DataLoader(prepared_amino_seq, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    codon_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            output = model(batch)  # (batch_size, seq_len, num_classes)\n",
    "            for batch_i in range(output.shape[0]):\n",
    "                predicted_codons = []\n",
    "                for seq_i in range(output.shape[1]):\n",
    "                    if SPEEDS_ADDED:\n",
    "                        aa_num = batch[batch_i][seq_i][0].item()\n",
    "                    else:\n",
    "                        aa_num = batch[batch_i][seq_i].item()\n",
    "                    if aa_num == aminoacids_to_integer['_']:\n",
    "                        break\n",
    "                    codon_idx = torch.argmax(output[batch_i][seq_i]).item()\n",
    "                    codon = integer_to_codons[codon_idx]\n",
    "                    predicted_codons.append(codon)\n",
    "                codon_predictions.append(predicted_codons)\n",
    "    codon_predictions = rebuild_sequences(codon_predictions, cut_bit_map)\n",
    "    return codon_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_seq = df['translation'].head()\n",
    "batched_predictions = predict_codons(model, amino_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids_to_codons = {\n",
    "    'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n",
    "    'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n",
    "    'N': ['AAT', 'AAC'],\n",
    "    'D': ['GAT', 'GAC'],\n",
    "    'C': ['TGT', 'TGC'],\n",
    "    'Q': ['CAA', 'CAG'],\n",
    "    'E': ['GAA', 'GAG'],\n",
    "    'G': ['GGT', 'GGC', 'GGA', 'GGG'],\n",
    "    'H': ['CAT', 'CAC'],\n",
    "    'I': ['ATT', 'ATC', 'ATA'],\n",
    "    'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n",
    "    'K': ['AAA', 'AAG'],\n",
    "    'M': ['ATG'],\n",
    "    'F': ['TTT', 'TTC'],\n",
    "    'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n",
    "    'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n",
    "    'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n",
    "    'W': ['TGG'],\n",
    "    'Y': ['TAT', 'TAC'],\n",
    "    'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n",
    "    '*': ['TAA', 'TAG', 'TGA']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weighted_bc = bc.Max_Bias_Baseline_Classifier(usage_biases)\n",
    "def check_and_replace_codons(aa_sequences, codon_predictions_list):\n",
    "    total_codons = 0\n",
    "    not_possible_codons = 0\n",
    "    used_max_bias = 0\n",
    "    for i, aa_seq in enumerate(aa_sequences):\n",
    "        codon_predictions = codon_predictions_list[i]\n",
    "        for j, aa in enumerate(aa_seq):\n",
    "            total_codons += 1\n",
    "            codon_pred = codon_predictions[j]\n",
    "            max_bias_pred = max_weighted_bc._predict_codon(aa)\n",
    "            if codon_pred not in amino_acids_to_codons[aa]:\n",
    "                not_possible_codons += 1\n",
    "                \n",
    "                codon_predictions_list[i][j] = max_bias_pred\n",
    "            else:\n",
    "                if codon_pred == max_bias_pred:\n",
    "                    used_max_bias += 1\n",
    "    max_bias_ratio = used_max_bias / total_codons\n",
    "    print(f\"Model used max bias codon for {max_bias_ratio*100:.2f}% of possible codon predictions\")\n",
    "    not_possible_ratio = not_possible_codons / total_codons\n",
    "    print(f\"Replaced {not_possible_ratio*100:.2f}% of codons\")\n",
    "    return codon_predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Classifier(Classifier.Classifier):\n",
    "    def __init__(self, trained_model, seed=42):\n",
    "        self.model = trained_model\n",
    "        super().__init__(seed)\n",
    "\n",
    "\n",
    "    def predict_codons(self, aa_sequences, replace=False):\n",
    "        predictions_list = predict_codons(self.model, aa_sequences)\n",
    "        if replace:\n",
    "            predictions_list = check_and_replace_codons(aa_sequences, predictions_list)\n",
    "        predictions_matrix = self.pad_and_convert_seq(predictions_list)\n",
    "        return predictions_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240522184144_encoder_256em_4l_4h_02dr_10ep_speeds.pt\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "\n",
    "model = EncoderClassifier(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    num_heads=NUM_HEADS\n",
    ").to(device)\n",
    "model = ml_helper.load_model('encoder_256em_4l_4h_02dr_10ep_speeds', organism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used max bias codon for 86.81% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "CPU times: user 8.24 s, sys: 16.5 ms, total: 8.26 s\n",
      "Wall time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "encoder_classifier = Encoder_Classifier(model)\n",
    "amino_seq = df['translation']\n",
    "true_codons = df['codons']\n",
    "pred_codons_replaced = encoder_classifier.predict_codons(amino_seq, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organismus E.Coli - Accuracy: 0.5215032682554798\n"
     ]
    }
   ],
   "source": [
    "accuracy = encoder_classifier.calc_accuracy(true_codons, pred_codons_replaced)\n",
    "print(f\"Organismus {organism} - Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organismus E.Coli - Accuracy: 0.5174658847557534\n"
     ]
    }
   ],
   "source": [
    "max_weighted_bc = bc.Max_Bias_Baseline_Classifier(usage_biases)\n",
    "amino_seq = df['translation'].apply(lambda seq: list(seq))\n",
    "true_codons = df['codons']\n",
    "pred_codons = max_weighted_bc.predict_codons(amino_seq)\n",
    "accuracy = max_weighted_bc.calc_accuracy(true_codons, pred_codons)\n",
    "print(f\"Organismus {organism} - Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_parameter_model(embed_dim, num_encoder_layers, num_heads, dropout, pos_enc, num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = EncoderClassifier(\n",
    "        embed_dim=embed_dim,\n",
    "        num_layers=num_encoder_layers,\n",
    "        num_heads=num_heads\n",
    "    ).to(device)\n",
    "    model = ml_helper.load_model( f'encoder_{embed_dim}em_{num_encoder_layers}l_{num_heads}h_{str(dropout).replace(\".\",\"\")}dr_{num_epochs}ep{\"_posenc\" if pos_enc else \"\"}', organism)\n",
    "\n",
    "    encoder_classifier = Encoder_Classifier(model)\n",
    "    amino_seq = df['translation']\n",
    "    true_codons = df['codons']\n",
    "    pred_codons_replaced = encoder_classifier.predict_codons(amino_seq, replace=True)\n",
    "\n",
    "    accuracy = round(encoder_classifier.calc_accuracy(true_codons, pred_codons_replaced), 4)\n",
    "    print(f\"Accuracy: {accuracy} - Organism: {organism}, Encoder Model - Parameters: {embed_dim} embedding dim, {num_encoder_layers} layers, {num_heads} heads, {num_epochs} epochs\")\n",
    "    print(f\"Took {round(time.time() - start_time, 2)} seconds\")\n",
    "    print(\"\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "POS_ENC = False\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240518114512_encoder_256em_4l_4h_01dr_10ep.pt\n",
      "Model used max bias codon for 91.08% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5229 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.41 seconds\n",
      "\n",
      "Model loaded: 20240518114547_encoder_256em_4l_4h_02dr_10ep.pt\n",
      "Model used max bias codon for 92.80% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.524 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.46 seconds\n",
      "\n",
      "Model loaded: 20240518114622_encoder_256em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.57% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5244 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.4 seconds\n",
      "\n",
      "Model loaded: 20240518114658_encoder_256em_4l_4h_04dr_10ep.pt\n",
      "Model used max bias codon for 92.65% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5239 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.31 seconds\n",
      "\n",
      "Model loaded: 20240518114733_encoder_256em_4l_4h_05dr_10ep.pt\n",
      "Model used max bias codon for 89.29% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5218 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.35 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_dropout = {}\n",
    "for DROPOUT in dropouts:\n",
    "    accuracy = eval_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, EPOCHS)\n",
    "    accuracies_dropout[f'encoder_{EMBED_DIM}em_{NUM_ENCODER_LAYERS}l_{NUM_HEADS}h_{str(DROPOUT).replace(\".\",\"\")}dr_{EPOCHS}ep{\"_posenc\" if POS_ENC else \"\"}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_256em_4l_4h_01dr_10ep': 0.5229,\n",
       " 'encoder_256em_4l_4h_02dr_10ep': 0.524,\n",
       " 'encoder_256em_4l_4h_03dr_10ep': 0.5244,\n",
       " 'encoder_256em_4l_4h_04dr_10ep': 0.5239,\n",
       " 'encoder_256em_4l_4h_05dr_10ep': 0.5218}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder_256em_4l_4h_03dr_10ep', 0.5244)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies_dropout.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.3\n",
    "pos_enc = [True, False]\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240518115201_encoder_256em_4l_4h_03dr_10ep_posenc.pt\n",
      "Model used max bias codon for 88.99% of possible codon predictions\n",
      "Replaced 0.05% of codons\n",
      "Accuracy: 0.5171 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.37 seconds\n",
      "\n",
      "Model loaded: 20240518115236_encoder_256em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.57% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5244 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.35 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_pos_enc = {}\n",
    "for POS_ENC in pos_enc:\n",
    "    accuracy = eval_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, EPOCHS)\n",
    "    accuracies_pos_enc[f'encoder_{EMBED_DIM}em_{NUM_ENCODER_LAYERS}l_{NUM_HEADS}h_{str(DROPOUT).replace(\".\",\"\")}dr_{EPOCHS}ep{\"_posenc\" if POS_ENC else \"\"}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_256em_4l_4h_03dr_10ep_posenc': 0.5171,\n",
       " 'encoder_256em_4l_4h_03dr_10ep': 0.5244}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder_256em_4l_4h_03dr_10ep', 0.5244)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies_dropout.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = [32, 64, 128, 256, 512, 1028]\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.3\n",
    "POS_ENC = False\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240518115409_encoder_32em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 100.00% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 32 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.3 seconds\n",
      "\n",
      "Model loaded: 20240518115430_encoder_64em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 91.54% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5225 - Organism: E.Coli, Encoder Model - Parameters: 64 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.26 seconds\n",
      "\n",
      "Model loaded: 20240518115453_encoder_128em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 94.71% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5233 - Organism: E.Coli, Encoder Model - Parameters: 128 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.36 seconds\n",
      "\n",
      "Model loaded: 20240518115529_encoder_256em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.57% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5244 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.42 seconds\n",
      "\n",
      "Model loaded: 20240518115634_encoder_512em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 93.21% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5213 - Organism: E.Coli, Encoder Model - Parameters: 512 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.63 seconds\n",
      "\n",
      "Model loaded: 20240518115901_encoder_1028em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 1028 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 8.17 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_emb = {}\n",
    "for EMBED_DIM in embed_dims:\n",
    "    accuracy = eval_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, EPOCHS)\n",
    "    accuracies_emb[f'encoder_{EMBED_DIM}em_{NUM_ENCODER_LAYERS}l_{NUM_HEADS}h_{str(DROPOUT).replace(\".\",\"\")}dr_{EPOCHS}ep{\"_posenc\" if POS_ENC else \"\"}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_32em_4l_4h_03dr_10ep': 0.5175,\n",
       " 'encoder_64em_4l_4h_03dr_10ep': 0.5225,\n",
       " 'encoder_128em_4l_4h_03dr_10ep': 0.5233,\n",
       " 'encoder_256em_4l_4h_03dr_10ep': 0.5244,\n",
       " 'encoder_512em_4l_4h_03dr_10ep': 0.5213,\n",
       " 'encoder_1028em_4l_4h_03dr_10ep': 0.5175}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder_256em_4l_4h_03dr_10ep', 0.5244)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies_emb.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number Encoder Layers and Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "num_encoder_layers = [2, 4, 8, 16]\n",
    "num_heads = [2, 4, 8, 16]\n",
    "DROPOUT = 0.3\n",
    "POS_ENC = False\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240518120122_encoder_256em_2l_2h_03dr_10ep.pt\n",
      "Model used max bias codon for 90.48% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5228 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 2 layers, 2 heads, 10 epochs\n",
      "Took 7.29 seconds\n",
      "\n",
      "Model loaded: 20240518120140_encoder_256em_2l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 91.66% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5241 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 2 layers, 4 heads, 10 epochs\n",
      "Took 7.22 seconds\n",
      "\n",
      "Model loaded: 20240518120201_encoder_256em_2l_8h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.38% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5238 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 2 layers, 8 heads, 10 epochs\n",
      "Took 7.28 seconds\n",
      "\n",
      "Model loaded: 20240518120228_encoder_256em_2l_16h_03dr_10ep.pt\n",
      "Model used max bias codon for 91.50% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5238 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 2 layers, 16 heads, 10 epochs\n",
      "Took 7.31 seconds\n",
      "\n",
      "Model loaded: 20240518120302_encoder_256em_4l_2h_03dr_10ep.pt\n",
      "Model used max bias codon for 87.70% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5174 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 2 heads, 10 epochs\n",
      "Took 7.41 seconds\n",
      "\n",
      "Model loaded: 20240518120338_encoder_256em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.57% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5244 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 7.38 seconds\n",
      "\n",
      "Model loaded: 20240518120419_encoder_256em_4l_8h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.83% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5241 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 8 heads, 10 epochs\n",
      "Took 7.39 seconds\n",
      "\n",
      "Model loaded: 20240518120513_encoder_256em_4l_16h_03dr_10ep.pt\n",
      "Model used max bias codon for 92.47% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.5237 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 4 layers, 16 heads, 10 epochs\n",
      "Took 7.49 seconds\n",
      "\n",
      "Model loaded: 20240518120621_encoder_256em_8l_2h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 8 layers, 2 heads, 10 epochs\n",
      "Took 7.6 seconds\n",
      "\n",
      "Model loaded: 20240518120731_encoder_256em_8l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 8 layers, 4 heads, 10 epochs\n",
      "Took 7.61 seconds\n",
      "\n",
      "Model loaded: 20240518120853_encoder_256em_8l_8h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 8 layers, 8 heads, 10 epochs\n",
      "Took 7.76 seconds\n",
      "\n",
      "Model loaded: 20240518121040_encoder_256em_8l_16h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 8 layers, 16 heads, 10 epochs\n",
      "Took 7.89 seconds\n",
      "\n",
      "Model loaded: 20240518121257_encoder_256em_16l_2h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 16 layers, 2 heads, 10 epochs\n",
      "Took 8.17 seconds\n",
      "\n",
      "Model loaded: 20240518121517_encoder_256em_16l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 16 layers, 4 heads, 10 epochs\n",
      "Took 8.17 seconds\n",
      "\n",
      "Model loaded: 20240518121800_encoder_256em_16l_8h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 16 layers, 8 heads, 10 epochs\n",
      "Took 8.35 seconds\n",
      "\n",
      "Model loaded: 20240518122134_encoder_256em_16l_16h_03dr_10ep.pt\n",
      "Model used max bias codon for 0.00% of possible codon predictions\n",
      "Replaced 100.00% of codons\n",
      "Accuracy: 0.5175 - Organism: E.Coli, Encoder Model - Parameters: 256 embedding dim, 16 layers, 16 heads, 10 epochs\n",
      "Took 8.68 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_layers_heads = {}\n",
    "for NUM_ENCODER_LAYERS in num_encoder_layers:\n",
    "    for NUM_HEADS in num_heads:\n",
    "        accuracy = eval_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, EPOCHS)\n",
    "        accuracies_layers_heads[f'encoder_{EMBED_DIM}em_{NUM_ENCODER_LAYERS}l_{NUM_HEADS}h_{str(DROPOUT).replace(\".\",\"\")}dr_{EPOCHS}ep{\"_posenc\" if POS_ENC else \"\"}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_256em_2l_2h_03dr_10ep': 0.5228,\n",
       " 'encoder_256em_2l_4h_03dr_10ep': 0.5241,\n",
       " 'encoder_256em_2l_8h_03dr_10ep': 0.5238,\n",
       " 'encoder_256em_2l_16h_03dr_10ep': 0.5238,\n",
       " 'encoder_256em_4l_2h_03dr_10ep': 0.5174,\n",
       " 'encoder_256em_4l_4h_03dr_10ep': 0.5244,\n",
       " 'encoder_256em_4l_8h_03dr_10ep': 0.5241,\n",
       " 'encoder_256em_4l_16h_03dr_10ep': 0.5237,\n",
       " 'encoder_256em_8l_2h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_8l_4h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_8l_8h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_8l_16h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_16l_2h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_16l_4h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_16l_8h_03dr_10ep': 0.5175,\n",
       " 'encoder_256em_16l_16h_03dr_10ep': 0.5175}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_layers_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder_256em_4l_4h_03dr_10ep', 0.5244)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies_layers_heads.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Drosophila.Melanogaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4335"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organism = \"Drosophila.Melanogaster\"\n",
    "\n",
    "def group_codons(sequence):\n",
    "    return [''.join(sequence[i:i+3]) for i in range(0, len(sequence), 3)]\n",
    "\n",
    "df = pd.read_pickle(f\"../data/{organism}/cleanedData_test.pkl\")\n",
    "usage_biases = pd.read_pickle(f\"../data/{organism}/usageBias.pkl\")\n",
    "df['codons'] = df['sequence'].apply(group_codons)\n",
    "\n",
    "min_length = None\n",
    "max_length = None\n",
    "\n",
    "df = ml_helper.filter_sequence_length(df, min_length, max_length)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.3\n",
    "POS_ENC = False\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 20240518122826_encoder_256em_4l_4h_03dr_10ep.pt\n",
      "Model used max bias codon for 88.39% of possible codon predictions\n",
      "Replaced 0.00% of codons\n",
      "Accuracy: 0.4074 - Organism: Drosophila.Melanogaster, Encoder Model - Parameters: 256 embedding dim, 4 layers, 4 heads, 10 epochs\n",
      "Took 107.46 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_parameter_model(EMBED_DIM, NUM_ENCODER_LAYERS, NUM_HEADS, DROPOUT, POS_ENC, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organismus Drosophila.Melanogaster - Accuracy: 0.49196268368089835\n"
     ]
    }
   ],
   "source": [
    "max_weighted_bc = bc.Max_Bias_Baseline_Classifier(usage_biases)\n",
    "amino_seq = df['translation'].apply(lambda seq: list(seq))\n",
    "true_codons = df['codons']\n",
    "pred_codons = max_weighted_bc.predict_codons(amino_seq)\n",
    "accuracy = max_weighted_bc.calc_accuracy(true_codons, pred_codons)\n",
    "print(f\"Organismus {organism} - Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
